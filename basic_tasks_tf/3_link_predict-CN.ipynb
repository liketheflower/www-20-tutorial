{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用GNN进行边预测\n",
    "\n",
    "GNNs are powerful tools for many machine learning tasks on graphs. This tutorial teaches the basic workflow of using GNNs for link prediction. We again use the Zachery's Karate Club graph but try to predict interactions between two members.\n",
    "\n",
    "In this tutorial, you will learn:\n",
    "* Prepare training and testing sets for link prediction task.\n",
    "* Build a GNN-based link prediction model.\n",
    "* Train the model and verify the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_utils import setup_tf\n",
    "setup_tf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入图结构和特征\n",
    "\n",
    "Following the last [session](./2_gnn-CN.ipynb), we first load the Zachery's Karate Club graph and creates node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=34, num_edges=156,\n",
      "      ndata_schemes={'club': Scheme(shape=(), dtype=tf.int64), 'club_onehot': Scheme(shape=(2,), dtype=tf.float32)}\n",
      "      edata_schemes={})\n",
      "<tf.Variable 'embedding/embeddings:0' shape=(34, 5) dtype=float32, numpy=\n",
      "array([[-0.38376212,  0.02762738,  0.2652063 ,  0.32293776,  0.04524353],\n",
      "       [ 0.30032715,  0.02468556, -0.1900916 , -0.04702508, -0.30461857],\n",
      "       [-0.1929569 ,  0.24494252, -0.38531214, -0.08113599,  0.06808767],\n",
      "       [ 0.02020505,  0.26825735, -0.3504967 ,  0.2496821 , -0.34984836],\n",
      "       [ 0.0258891 , -0.15108845, -0.35368958,  0.37582836, -0.29545236],\n",
      "       [ 0.2677717 ,  0.08830869,  0.28496173,  0.02015099,  0.05002049],\n",
      "       [-0.00256091,  0.10553828, -0.10098866, -0.25102186,  0.20928678],\n",
      "       [ 0.23899326, -0.27900234,  0.23708245, -0.20309108, -0.11720824],\n",
      "       [-0.07901543,  0.31122229,  0.01442784,  0.03468132, -0.16346353],\n",
      "       [ 0.00062189,  0.32725772,  0.22976199, -0.09203568,  0.0605621 ],\n",
      "       [ 0.18852326, -0.22114322,  0.01407388,  0.02442989, -0.07274896],\n",
      "       [-0.19398539, -0.1850395 , -0.28360528, -0.12099096, -0.17968921],\n",
      "       [-0.00870264, -0.29916045, -0.0631997 ,  0.15708569,  0.07021001],\n",
      "       [ 0.3636075 , -0.00296146,  0.00969708,  0.32759282,  0.13714948],\n",
      "       [ 0.03305024,  0.10574755, -0.2602409 ,  0.29580548, -0.11125207],\n",
      "       [-0.11671582,  0.11565533,  0.3492187 ,  0.293992  ,  0.37837824],\n",
      "       [ 0.3312573 , -0.28587145, -0.03252721,  0.26750275,  0.28610566],\n",
      "       [ 0.12475982, -0.30301076, -0.24551296,  0.29956558, -0.18915391],\n",
      "       [-0.11880317, -0.24317536,  0.2580764 ,  0.22547969,  0.22258344],\n",
      "       [ 0.2329677 ,  0.10412478, -0.28541106,  0.37384805, -0.02423584],\n",
      "       [ 0.2116668 , -0.22576325,  0.07006133, -0.19694842, -0.30447578],\n",
      "       [-0.38416442,  0.07963702, -0.22657318,  0.2628081 , -0.20528664],\n",
      "       [-0.2577273 , -0.23448086,  0.32212886, -0.24628036, -0.3447023 ],\n",
      "       [-0.3161682 , -0.19704661, -0.19434355, -0.01621708, -0.30884323],\n",
      "       [ 0.37599108,  0.0297989 ,  0.06864455,  0.03377843,  0.23777828],\n",
      "       [ 0.08998618,  0.3604851 , -0.099262  ,  0.10451549, -0.07155886],\n",
      "       [ 0.22283366, -0.3278796 ,  0.1046724 , -0.09897768,  0.23841599],\n",
      "       [-0.1946501 ,  0.11716357, -0.32857564,  0.10770017, -0.1645889 ],\n",
      "       [ 0.17030624, -0.36210936,  0.01169559,  0.10552621,  0.36919817],\n",
      "       [ 0.19632813,  0.36338618,  0.36978415,  0.20446762, -0.18749607],\n",
      "       [-0.27377343,  0.00047234,  0.18350473,  0.37348035,  0.23951712],\n",
      "       [-0.26527306,  0.1634542 , -0.17902365,  0.38443735, -0.17161882],\n",
      "       [-0.15571484,  0.08100843,  0.26142022,  0.02388862,  0.22822693],\n",
      "       [ 0.19441465,  0.07608762, -0.24678141, -0.22402386, -0.19314057]],\n",
      "      dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "from tutorial_utils import load_zachery\n",
    "\n",
    "# ----------- 0. load graph -------------- #\n",
    "g = load_zachery()\n",
    "print(g)\n",
    "\n",
    "# ----------- 1. node features -------------- #\n",
    "node_embed = tf.keras.layers.Embedding(g.number_of_nodes(), 5,\n",
    "                                       embeddings_initializer='glorot_uniform')  # Every node has an embedding of size 5.\n",
    "node_embed(1) # intialize embedding layer\n",
    "inputs = node_embed.embeddings # # Use the embedding weight as the node features.\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备训练和测试集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a link prediction data set contains two types of edges, *positive* and *negative edges*. Positive edges are usually drawn from the existing edges in the graph. In this example, we randomly pick 50 edges for testing and leave the rest for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split edge set for training and testing\n",
    "u, v = g.edges()\n",
    "u, v = u.numpy(), v.numpy()\n",
    "eids = np.arange(g.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_pos_u, test_pos_v = u[eids[:50]], v[eids[:50]]\n",
    "train_pos_u, train_pos_v = u[eids[50:]], v[eids[50:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of negative edges is large, sampling is usually desired. How to choose proper negative sampling algorithms is a widely-studied topic and is out of scope of this tutorial. Since our example graph is quite small (with only 34 nodes), we enumerate all the missing edges and randomly pick 50 for testing and 150 for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all negative edges and split them for training and testing\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u, v)))\n",
    "adj_neg = 1 - adj.todense() - np.eye(34)\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "neg_eids = np.random.choice(len(neg_u), 200)\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:50]], neg_v[neg_eids[:50]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[50:]], neg_v[neg_eids[50:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put positive and negative edges together and form training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set.\n",
    "train_u = tf.concat([train_pos_u, train_neg_u], axis=0)\n",
    "train_v = tf.concat([train_pos_v, train_neg_v], axis=0)\n",
    "train_label = tf.concat([tf.zeros(len(train_pos_u)), tf.ones(len(train_neg_u))], axis=0)\n",
    "\n",
    "# Create testing set.\n",
    "test_u = tf.concat([test_pos_u, test_neg_u], axis=0)\n",
    "test_v = tf.concat([test_pos_v, test_neg_v], axis=0)\n",
    "test_label = tf.concat([tf.zeros(len(test_pos_u)), tf.ones(len(test_neg_u))], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义GraphSAGE的模型\n",
    "\n",
    "Our model consists of two layers, each computes new node representations by aggregating neighbor information. The equations are:\n",
    "\n",
    "$$\n",
    "h_{\\mathcal{N}(v)}^k\\leftarrow \\text{AGGREGATE}_k\\{h_u^{k-1},\\forall u\\in\\mathcal{N}(v)\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_v^k\\leftarrow \\text{ReLU}\\left(W^k\\cdot \\text{CONCAT}(h_v^{k-1}, h_{\\mathcal{N}(v)}^k) \\right)\n",
    "$$\n",
    "\n",
    "DGL provides implementation of many popular neighbor aggregation modules. They all can be invoked easily with one line of codes. See the full list of supported [graph convolution modules](https://docs.dgl.ai/api/python/nn.pytorch.html#module-dgl.nn.pytorch.conv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# build a two-layer GraphSAGE model\n",
    "class GraphSAGE(tf.keras.layers.Layer):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "    \n",
    "    def call(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "# Create the model with given dimensions \n",
    "# input layer dimension: 5, node embeddings\n",
    "# hidden layer dimension: 16\n",
    "net = GraphSAGE(5, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对边预测使用针对性的损失函数\n",
    "\n",
    "We then optimize the model using the following loss function.\n",
    "\n",
    "$$\n",
    "\\hat{y}_{u\\sim v} = \\sigma(h_u^T h_v)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{u\\sim v\\in \\mathcal{D}}\\left( y_{u\\sim v}\\log(\\hat{y}_{u\\sim v}) + (1-y_{u\\sim v})\\log(1-\\hat{y}_{u\\sim v})) \\right)\n",
    "$$\n",
    "\n",
    "Essentially, the model predicts a score for each edge by dot-producting the representations of its two end-points. It then computes a binary cross entropy loss with the target $y$ being 0 or 1 meaning whether the edge is a positive one or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jamezhan/anaconda3/envs/dgl/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:644: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.identity instead.\n",
      "In epoch 0, loss: 0.6808468699455261\n",
      "In epoch 5, loss: 0.5773298740386963\n",
      "In epoch 10, loss: 0.44662487506866455\n",
      "In epoch 15, loss: 0.39926254749298096\n",
      "In epoch 20, loss: 0.3401777446269989\n",
      "In epoch 25, loss: 0.32041066884994507\n",
      "In epoch 30, loss: 0.2868993878364563\n",
      "In epoch 35, loss: 0.2574978470802307\n",
      "In epoch 40, loss: 0.23335880041122437\n",
      "In epoch 45, loss: 0.20876789093017578\n",
      "In epoch 50, loss: 0.1869775354862213\n",
      "In epoch 55, loss: 0.16242516040802002\n",
      "In epoch 60, loss: 0.1356896162033081\n",
      "In epoch 65, loss: 0.11280933022499084\n",
      "In epoch 70, loss: 0.08605014532804489\n",
      "In epoch 75, loss: 0.054293327033519745\n",
      "In epoch 80, loss: 0.02977687492966652\n",
      "In epoch 85, loss: 0.013111919164657593\n",
      "In epoch 90, loss: 0.005503080785274506\n",
      "In epoch 95, loss: 0.0021488661877810955\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fcn = tf.keras.losses.BinaryCrossentropy(\n",
    "    from_logits=False)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(inputs) # optimize embedding layer also\n",
    "        # forward\n",
    "        logits = net(g, inputs)\n",
    "        pred = tf.sigmoid(tf.reduce_sum(tf.gather(logits, train_u) *\n",
    "                                        tf.gather(logits, train_v), axis=1))\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_fcn(train_label, pred)\n",
    "\n",
    "        # backward\n",
    "        grads = tape.gradient(loss, net.trainable_weights + node_embed.trainable_weights)        \n",
    "        optimizer.apply_gradients(zip(grads, net.trainable_weights + node_embed.trainable_weights))\n",
    "        all_logits.append(logits.numpy())\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.8\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. check results ------------------------ #\n",
    "pred = tf.sigmoid(tf.reduce_sum(tf.gather(logits, test_u) *\n",
    "                                tf.gather(logits, test_v), axis=1)).numpy()\n",
    "print('Accuracy', ((pred >= 0.5) == test_label.numpy()).sum().item() / len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dgl]",
   "language": "python",
   "name": "conda-env-dgl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
